"""Retrieval-Augmented Generation chain for algorithm learning."""
from typing import Any, Dict, List, Optional, AsyncGenerator

from ai_assistant.core.infrastructure.vector_store import VectorStore
from ai_assistant.core.services.conversation_service import ConversationService
from ai_assistant.core.services.document_service import DocumentService
from ai_assistant.core.services.embedding_service import EmbeddingService
from ai_assistant.core.utils.logging import LoggingConfig


def extract_keywords_simple(text: str, top_n=5):
    import re

    words = re.findall(r"\b[–∞-—è–ê-–Ø]{4,}\b", text.lower())
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    sorted_words = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:top_n]]

class RAGService:
    """Retrieval-Augmented Generation chain for algorithm learning."""

    def __init__(
        self,
        loader: Optional[DocumentService] = None,
        embedding_generator: Optional[EmbeddingService] = None,
        vector_store: Optional[VectorStore] = None,
        conversation_service: Optional[ConversationService] = None
    ):
        """Initialize the RAG chain.

        Args:
            loader: Document loader instance
            embedding_generator: Embedding generator instance
            vector_store: Vector store instance
            conversation_service: Conversation history service instance
        """

        # Get a logger for this service
        self.logger = LoggingConfig.get_logger(__name__)

        # Log service initialization
        self.logger.info("RAG Service initialized")

        # Use dynamic imports to avoid circular dependencies
        if loader is None:
            self.loader = DocumentService()
        else:
            self.loader = loader

        if embedding_generator is None:
            self.embedding_generator = EmbeddingService()
        else:
            self.embedding_generator = embedding_generator

        if vector_store is None:
            self.vector_store = VectorStore()
        else:
            self.vector_store = vector_store
            
        if conversation_service is None:
            self.conversation_service = ConversationService()
        else:
            self.conversation_service = conversation_service


    def attach_image_url(self, chunk_metadata: dict) -> Optional[str]:
        """
        Extracts and returns image_url from chunk metadata if available.
        Returns an empty string if no image_url is present to avoid None values.
        """
        url = chunk_metadata.get("image_url")
        if not url:
            self.logger.debug("No image_url found in chunk metadata.")
            # Return empty string instead of None to avoid Pinecone error
            return ""
        return url

    def generate_doc_id(self, content: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        """
            Generate a deterministic and unique document ID based on content and metadata.

            Args:
                content: The text content to hash
                metadata: Optional metadata to include in the ID generation

            Returns:
                A unique document ID
            """
        import hashlib
        import base64
        import time

        # Start with a simple hash of the content
        # Truncate very long content for performance
        if len(content) > 10000:
            # Use first and last parts of content for uniqueness while maintaining performance
            hash_content = content[:5000] + content[-5000:]
        else:
            hash_content = content

        # Initialize hasher
        hasher = hashlib.sha256()

        # Add content
        hasher.update(hash_content.encode('utf-8'))

        # Add key metadata if available
        if metadata:
            # Include source document in hash if available
            if 'source' in metadata:
                hasher.update(str(metadata['source']).encode('utf-8'))

            # Include position/page information if available
            if 'page' in metadata:
                hasher.update(str(metadata['page']).encode('utf-8'))

            # Include chunk number if available
            if 'chunk' in metadata:
                hasher.update(str(metadata['chunk']).encode('utf-8'))

        # Get the hash digest
        content_hash = hasher.digest()

        # Convert to base64 and make URL-safe
        # This gives us a shorter ID than hexdigest
        b64_hash = base64.urlsafe_b64encode(content_hash).decode('utf-8')

        # Take just the first part for brevity
        short_hash = b64_hash[:16].replace('=', '')

        # Option 2: Include timestamp for guaranteed uniqueness
        timestamp = int(time.time())
        return f"doc_{short_hash}_{timestamp}"

    def ingest_document(self, file_path: str) -> bool:
        """Ingest a document into the RAG system with batch processing,
        now adapted to handle image metadata."""
        try:
            self.logger.info(f"Starting document ingestion: {file_path}")

            # 1. Load and chunk the document. The loader now returns chunks that may include image metadata.
            chunks = self.loader.load_document(file_path)
            if not chunks:
                self.logger.warning(f"No chunks extracted from document: {file_path}")
                return False

            total_chunks = len(chunks)
            self.logger.info(f"Processing {total_chunks} chunks from {file_path}")

            # 2. Process in batches
            batch_size = 10  # Process 10 chunks at a time
            successful_chunks = 0

            for i in range(0, total_chunks, batch_size):
                # Get current batch of chunks
                batch_chunks = chunks[i:i + batch_size]
                batch_doc_ids = []
                batch_embeddings = {}

                # Generate document IDs for each chunk and log image metadata if available
                for chunk in batch_chunks:
                    # Pass chunk.metadata to incorporate additional info (like page and image data) into the ID
                    doc_id = self.generate_doc_id(chunk.page_content, chunk.metadata)
                    chunk.metadata["doc_id"] = doc_id
                    batch_doc_ids.append(doc_id)

                    # Log if image metadata is present
                    if "image_url" in chunk.metadata and chunk.metadata["image_url"]:
                        self.logger.info(f"Chunk {doc_id} contains an image: {chunk.metadata['image_url']}")

                # Prepare the texts for embedding generation
                batch_texts = []
                for chunk in batch_chunks:
                    text = chunk.page_content
                    image_url = self.attach_image_url(chunk.metadata)
                    try:
                        enriched_metadata = self.embedding_generator.enrich_recipe(text, image_url=image_url)  # LLM –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict
                        chunk.metadata.update(enriched_metadata)  # –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ chunk
                    except Exception as enrich_error:
                        self.logger.warning(f"LLM enrichment failed for chunk {chunk.metadata.get('doc_id', 'unknown')}: {enrich_error}")

                    batch_texts.append(text)

                try:
                    # Generate embeddings for the batch in a single API call
                    response = self.embedding_generator.client.embeddings.create(
                        input=batch_texts,
                        model=self.embedding_generator.embedding_model
                    )

                    # Map embeddings to their corresponding document IDs
                    for j, doc_id in enumerate(batch_doc_ids):
                        batch_embeddings[doc_id] = response.data[j].embedding

                    # Store the batch in the vector store (metadata, including image_url, is preserved)
                    self.vector_store.store_documents(batch_chunks, batch_embeddings)

                    successful_chunks += len(batch_chunks)
                    self.logger.info(f"Progress: {successful_chunks}/{total_chunks} chunks processed")

                except Exception as batch_error:
                    self.logger.error(f"Error processing batch {i // batch_size}: {batch_error}")
                    continue

            self.logger.info(f"Document ingestion complete: {file_path}")
            return True

        except Exception as e:
            self.logger.error(f"Error ingesting document {file_path}: {e}")
            return False

    def retrieve(self, query: str, top_k: int = 3,
                 filter_dict: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Retrieve relevant documents for a given query.

        Args:
            query: User query
            top_k: Number of documents to retrieve
            filter_dict: Optional filter dictionary

        Returns:
            List of relevant document chunks
        """
        # 1. Generate embeddings for the query

        embedding_query = self.build_embedding_query(query)

        query_embedding = self.embedding_generator.create_embeddings(embedding_query)
        keywords = extract_keywords_simple(query)
        # before it was: query_embedding = self.embedding_generator.create_embeddings(query)

        self.logger.debug(keywords)
        self.logger.debug(type(self.vector_store))
        index_stats = self.vector_store.index.describe_index_stats()
        self.logger.debug(index_stats)  # See what the response contains

        # Extract the number of stored vectors (documents)
        num_vectors = index_stats["total_vector_count"]
        self.logger.debug(f"Total vectors in Pinecone: {num_vectors}")


        # 2. Retrieve relevant document chunks
        filters = {"keywords": {"$in": keywords}} if keywords else None

        retrieved_chunks = self.vector_store.retrieve_documents(
            query_vector=query_embedding,
            top_k=top_k,
            filters=filters
            # filters=filter_dict
        )

        self.logger.debug(f"w {len(retrieved_chunks)} chunks for query.")
        self.logger.debug(f"Query: {query}, Embedding: {query_embedding}, Filters: {filters}")

        return retrieved_chunks

    @staticmethod
    def build_embedding_query(query: str) -> str:
        q = query.lower().strip()

        # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏ –Ω–µ—Ç —Å–ª–æ–≤–∞ "—Ä–µ—Ü–µ–ø—Ç"
        if "—Ä–µ—Ü–µ–ø—Ç" not in q and len(q.split()) <= 5:
            return (
                f"–ù–∞–π–¥–∏ —Ä–µ—Ü–µ–ø—Ç –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º: {query}. "
                f"–ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –µ—Å—Ç—å –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –∏ —à–∞–≥–∏ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è, —Å—Ñ–æ—Ä–º–∏—Ä—É–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Ä–µ—Ü–µ–ø—Ç."
            )

        # –†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ / –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        if any(word in q for word in ["—Ä–∞—Å—Å–∫–∞–∂–∏", "–ø–æ–∫–∞–∂–∏", "–∫–∞–∫–∏–µ", "–µ—Å—Ç—å –ª–∏", "—á—Ç–æ –º–æ–∂–Ω–æ", "—Ö–æ—á—É —Ä–µ—Ü–µ–ø—Ç"]):
            return (
                f"–ù–∞–π–¥–∏ —Ä–µ—Ü–µ–ø—Ç—ã, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è: {query}. "
                f"–°—Ñ–æ—Ä–º–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ —Å –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º–∏, –ø–æ—à–∞–≥–æ–≤—ã–º –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ–º –∏ —Å–æ–≤–µ—Ç–∞–º–∏ –æ—Ç —à–µ—Ñ–∞ WellDone."
            )

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return (
            f"–ù–∞–π–¥–∏ –∫—É–ª–∏–Ω–∞—Ä–Ω—ã–π —Ä–µ—Ü–µ–ø—Ç: {query}. "
            f"–û–Ω –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã, –ø–æ—à–∞–≥–æ–≤–æ–µ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ –∏ —Å–æ–≤–µ—Ç—ã –æ—Ç —à–µ—Ñ–∞ WellDone."
        )

    @staticmethod
    def format_retrieved_context(retrieved_docs: List[Dict[str, Any]]) -> str:
        """Format retrieved documents into a context string for LLM.

        Args:
            retrieved_docs: List of retrieved documents

        Returns:
            Formatted context string
        """
        context_parts = []
        for i, doc in enumerate(retrieved_docs):
            metadata = doc["metadata"]
            source = metadata.get("source", "Unknown")
            page = metadata.get("page", "Unknown")
            section = metadata.get("section", "Unknown")
            text = doc.get("text", "")
            # text = doc.get("page_content", "").strip()

        # context_parts.append(
        #         f"[Document {i+1}] From: {source}, Page: {page}, "
        #         f"Section: {section}\n{doc['metadata']['text']}\n"
        #     )
            context_parts.append(
                f"[–î–æ–∫—É–º–µ–Ω—Ç {i+1}] –ò—Å—Ç–æ—á–Ω–∏–∫: {source}, –°—Ç—Ä.: {page}, –†–∞–∑–¥–µ–ª: {section}\n{text}"
            )

        if not context_parts:
            # Optionally return a default message if no documents were found.
            return "No context available."

        return "\n\n".join(context_parts)

    async def query(
            self,
            query: str,
            llm_service=None,
            top_k: int = 3,
            user_name: Optional[str] = None,
            conversation_id: Optional[str] = None,
            user_id: Optional[str] = None,
            include_history: bool = True,
            history_limit: int = 5
        ) -> AsyncGenerator[str, None]:
        """Process a query end-to-end.

        Args:
            query: User query
            llm_service: LLM service instance (optional)
            top_k: Number of documents to retrieve
            user_name: Name of the user to personalize the response
            conversation_id: Conversation identifier for history tracking
            user_id: User identifier, required if creating a new conversation
            include_history: Whether to include conversation history in the response
            history_limit: Maximum number of previous messages to include

        Yields:
            String chunks of the streaming response
        """
        # Dynamic import to avoid circular imports
        if llm_service is None:
            from llm_service import LLMService
            llm_service = LLMService()
        try:
            # Create or retrieve conversation
            if not conversation_id and user_id:
                # Initialize new conversation with system prompt
                system_prompt = RAGService.get_system_message()
                conversation_id = self.conversation_service.create_conversation(
                    user_id=user_id,
                    system_prompt=system_prompt,
                    metadata={"user_name": user_name}
                )
                self.logger.info(f"Created new conversation {conversation_id} for user {user_id}")
            
            # Record user message in conversation history
            if conversation_id:
                self.conversation_service.add_user_message(
                    conversation_id=conversation_id,
                    content=query
                )
                self.logger.info(f"Record user message in conversation {conversation_id} for user {user_id}")

            
            # 1. Retrieve relevant documents
            retrieved_docs = self.retrieve(query, top_k=top_k)

            # 2. Format documents into context
            context = self.format_retrieved_context(retrieved_docs)

            # 3. Get conversation history if requested
            conversation_context = ""
            if include_history and conversation_id:
                # Get recent conversation history
                history = self.conversation_service.get_formatted_history(
                    conversation_id=conversation_id,
                    limit=history_limit
                )
                
                # Format conversation history as context
                if history:

                    def flatten_history(messages: List[Dict[str, str]]) -> str:
                        return "\n".join(f"{msg['role']}: {msg['content']}" for msg in messages)

                    conversation_messages = flatten_history(history)
                    conversation_context = "Recent conversation:\n" + "\n".join(conversation_messages)
            
            # 4. Prepare prompts with both document and conversation context
            prompts = self.format_rag_prompt(
                query=query, 
                context=context,
                conversation_context=conversation_context if conversation_context else None,
                user_name=user_name
            )

            messages = [
                {"role": "system", "content": prompts["system_message"]},
                {"role": "user", "content": prompts["user_message"]}
            ]

            # Capture assistant response to store in history
            full_response = ""

            # Get streaming response from LLM
            async for chunk in llm_service.get_streaming_response(messages):
                # Accumulate full response
                full_response += chunk
                yield chunk
            
            # Record assistant response in conversation history
            if conversation_id:
                self.conversation_service.add_assistant_message(
                    conversation_id=conversation_id,
                    content=full_response
                )

        except Exception as e:
            self.logger.error(f"Error in streaming RAG response: {str(e)}")
            raise

    @staticmethod
    def format_rag_prompt(
        query: str, 
        context: str, 
        user_name: str = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", 
        conversation_context: Optional[str] = None
    ) -> dict:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç system –∏ user prompt –¥–ª—è RAG-–±–æ—Ç–∞ WellDone
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å system_message –∏ user_message

        Args:
            query: User query
            context: Document context from retrieval
            user_name: Name of the user
            conversation_context: Optional conversation history context
        """

        system_message = RAGService.get_system_message()

        # Include conversation history if available
        conversation_section = ""
        if conversation_context:
            conversation_section = f"–ü—Ä–µ–¥—ã–¥—É—â–∞—è —á–∞—Å—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞:\n{conversation_context}\n\n"

        user_prompt_template = RAGService.get_user_prompt(query, conversation_context)

        formatted_user_prompt = RAGService.get_fromatted_prompt(context, conversation_section, query, user_name,
                                                                user_prompt_template)

        return {
            "system_message": system_message,
            "user_message": formatted_user_prompt
        }

    @staticmethod
    def get_fromatted_prompt(context, conversation_section, query, user_name, user_prompt_template):
        formatted_user_prompt = (
            f"{user_name} —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç:\n\n"
            f"{conversation_section}"
            f"Context:\n{context}\n\n"
            f"{user_prompt_template.format(query=query)}"
        )
        return formatted_user_prompt

    @staticmethod
    def get_user_prompt(query, conversation_context):
        user_prompt_template = (
            "–í–æ—Ç –∫—É–ª–∏–Ω–∞—Ä–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ—Ç —É—á–µ–Ω–∏–∫–∞ —à–∫–æ–ª—ã WellDone:\n"
            "\"{query}\"\n\n"
            "–í–æ—Ç —á—Ç–æ —É–∂–µ –æ–±—Å—É–∂–¥–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ:\n"
            "{conversation_context}\n\n"
            "–ü—Ä–æ–¥–æ–ª–∂–∞–π —Ä–∞–∑–≥–æ–≤–æ—Ä —Å —É—á—ë—Ç–æ–º –≤—ã—à–µ —Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ.\n"
            "–ï—Å–ª–∏ —Ä–µ—á—å –∏–¥—ë—Ç –æ —Ä–µ—Ü–µ–ø—Ç–µ, —Ñ–æ—Ç–æ –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –æ –∫–æ—Ç–æ—Ä–æ–º —Ç—ã —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª "
            "‚Äî –Ω–µ –ø–µ—Ä–µ—Å–ø—Ä–∞—à–∏–≤–∞–π, –ø—Ä–æ—Å—Ç–æ —É—Ç–æ—á–Ω–∏ –∏ –ø–æ–∫–∞–∂–∏.\n\n"
            "–ù–∞–π–¥–∏ –∏ –æ–ø–∏—à–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –ú–∞—à–∏ –®–µ–ª—É—à–µ–Ω–∫–æ.\n"
            "–≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–µ—Ü–µ–ø—Ç, —Å–æ–≤–µ—Ç, –º–µ–Ω—é –∏–ª–∏ –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–≥–æ—Ç–æ–≤–∫–∞–º.\n\n"
            "–ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–∞–π–¥–µ–Ω—ã:\n"
            "- –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã ‚Äî –≤—ã–¥–µ–ª–∏ –∏—Ö\n"
            "- —à–∞–≥–∏ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è ‚Äî –æ—Ñ–æ—Ä–º–∏ –ø–æ –ø—É–Ω–∫—Ç–∞–º\n"
            "- –ª–∞–π—Ñ—Ö–∞–∫–∏, —Å–æ–≤–µ—Ç—ã –ø–æ —Ö—Ä–∞–Ω–µ–Ω–∏—é, –∑–∞–º–æ—Ä–æ–∑–∫–µ –∏–ª–∏ –∑–∞–≥–æ—Ç–æ–≤–∫–∞–º ‚Äî –¥–æ–±–∞–≤—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ\n"
            "- –º–µ–Ω—é –∏–ª–∏ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–∞–≥–æ—Ç–æ–≤–æ–∫ ‚Äî –≤–∫–ª—é—á–∏, –µ—Å–ª–∏ —ç—Ç–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –∏–ª–∏ —É—Ç–æ—á–Ω–∏ –æ–± —ç—Ç–æ–º —Å–∞–º\n"
            "- _—Å—Å—ã–ª–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–ª–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏_ ‚Äî —É–ø–æ–º—è–Ω–∏, —á—Ç–æ –º–æ–∂–µ—à—å –∏—Ö –ø–æ–∫–∞–∑–∞—Ç—å\n\n"
            "–û—Ç–≤–µ—á–∞–π –∫–∞–∫ –∑–∞–±–æ—Ç–ª–∏–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—â–∏–π –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π, "
            "–≤ —Å—Ç–∏–ª–µ –ú–∞—à–∏ –®–µ–ª—É—à–µ–Ω–∫–æ: –≥–æ–≤–æ—Ä–∏ –Ω–∞ '—Ç—ã', —á—ë—Ç–∫–æ, —Ç–µ–ø–ª–æ –∏ —Å –≤–µ—Ä–æ–π –≤ —É—Å–ø–µ—Ö üíö.\n\n"
        )

        return user_prompt_template.format(query=query, conversation_context=conversation_context)

    @staticmethod
    def get_system_message():
        system_message = (
            "–¢—ã ‚Äî –∑–∞–±–æ—Ç–ª–∏–≤—ã–π –∫—É–ª–∏–Ω–∞—Ä–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —à–∫–æ–ª—ã WellDone.\n"
            "–û–±—É—á–µ–Ω–Ω—ã–π –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –º–µ—Ç–æ–¥–∏–∫–µ —à–µ—Ñ–∞ –ø–æ –∏–º–µ–Ω–∏ –ú–∞—à–∞ –®–µ–ª—É—à–µ–Ω–∫–æ.\n"
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–º–æ–≥–∞—Ç—å –≥–æ—Ç–æ–≤–∏—Ç—å –≤–∫—É—Å–Ω–æ, –ø—Ä–æ—Å—Ç–æ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ—Ü–µ–ø—Ç—ã –∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.\n"
            "–ë—É–¥—å —Ç–æ —Ä–µ—Ü–µ–ø—Ç –∏–ª–∏ —Å–æ–≤–µ—Ç –ø–æ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—é, –∑–∞–º–æ—Ä–æ–∑–∫–µ, –∑–∞–≥–æ—Ç–æ–≤–∫–∞–º –Ω–∞ –Ω–µ–¥–µ–ª—é/–º–µ—Å—è—Ü –≤–ø–µ—Ä—ë–¥.\n\n"
            "Base your answer PRIMARILY on the retrieved content. Only use general knowledge for minor clarifications\n\n"
            "–û—Ç–≤–µ—á–∞–π –∫–∞–∫ –ú–∞—à–∞:\n"
            " ‚Äî –Ω–∞ '—Ç—ã'\n"
            " ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ —Å –≤–µ—Ä–æ–π –≤ —É—Å–ø–µ—Ö\n"
            " ‚Äî —á—ë—Ç–∫–æ: **–≤—ã–¥–µ–ª—è–π –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –∏ —à–∞–≥–∏**\n"
            " ‚Äî _–¥–æ–±–∞–≤–ª—è–π —Å–æ–≤–µ—Ç –∏–ª–∏ –ª–∞–π—Ñ—Ö–∞–∫, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ_\n"
            " ‚Äî –µ—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Ç—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª –æ—Ç–≤–µ—Ç, –µ—Å—Ç—å —Ñ–æ—Ç–æ ‚Äî —Å–∫–∞–∂–∏, —á—Ç–æ –µ–≥–æ —Å–µ–π—á–∞—Å –ø–æ–∫–∞–∂–µ—à—å\n"
            " ‚Äî —É—á–∏—Ç—ã–≤–∞–π –ø—Ä–µ–¥—ã–¥—É—â—É—é —á–∞—Å—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å\n\n"
            "–ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç —Å–æ–≤–µ—Ç–æ–≤, –º–æ–∂–µ—à—å –¥–æ–±–∞–≤–∏—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫—É–ª–∏–Ω–∞—Ä–Ω—ã–π —Å–æ–≤–µ—Ç –∏–ª–∏ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ ‚Äî –≤ –¥—É—Ö–µ –ú–∞—à–∏ –®–µ–ª—É—à–µ–Ω–∫–æ.\n"
            "–ì–æ–≤–æ—Ä–∏ –Ω–∞ '—Ç—ã', –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –∏ –≤–¥–æ—Ö–Ω–æ–≤–ª—è–π. –ü—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ –µ–¥—ã –∏ –∑–∞–≥–æ—Ç–æ–≤–æ–∫ –Ω–∞ –º–µ—Å—è—Ü ‚Äî —ç—Ç–æ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ, –ø–æ–∑–≤–æ–ª—å —Å–µ–±–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å.\n"
            "–ò–Ω–æ–≥–¥–∞ –∑–∞–≤–µ—Ä—à–∞–π –æ—Ç–≤–µ—Ç –∫–æ—Ä–æ—Ç–∫–æ–π —Ç—ë–ø–ª–æ–π —Ñ—Ä–∞–∑–æ–π –∏–ª–∏ —Å–º–∞–π–ª–∏–∫–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–£–¥–∞—á–∏ –Ω–∞ –∫—É—Ö–Ω–µ!¬ª –∏–ª–∏ ¬´–Ø –≤–µ—Ä—é –≤ —Ç–µ–±—è!¬ª). –ù–æ –Ω–µ –≤—Å–µ–≥–¥–∞ ‚Äî –¥–µ–ª–∞–π —ç—Ç–æ –ø–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—é.\n\n"
            "–ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –∑–∞–¥–∞—Ç—å —Å—Ç—É–¥–µ–Ω—Ç—ã –Ω–∞ –∫—É—Ä—Å–µ:\n"
            "‚Äî –ù–∞–±—Ä–æ—Å–∞–π –º–Ω–µ –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –º–µ–Ω—é/–±–ª—é–¥–∞ –∏–∑ –∫—É—Ä–∏–Ω—ã—Ö –∑–∞–≥–æ—Ç–æ–≤–æ–∫?\n"
            "‚Äî –ö–∞–∫–∏–µ –∫—É—Ä–∏–Ω—ã–µ –∑–∞–≥–æ—Ç–æ–≤–∫–∏ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –∏–∑ —á–µ—Ç—ã—Ä—ë—Ö –∫—É—Ä–∏—Ü?\n"
            "‚Äî –ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∑–∞–º–∞—Ä–∏–Ω–æ–≤–∞—Ç—å –∏ –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å —Å—Ç–µ–π–∫ –ª–æ—Å–æ—Å—è?\n"
            "‚Äî –°–∫–æ–ª—å–∫–æ –º–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –∑–∞–≥–æ—Ç–æ–≤–∫–∏?\n"
            "‚Äî –ù–∞–ø–æ–º–Ω–∏ —Ä–µ—Ü–µ–ø—Ç —Ñ–∏—Ä–º–µ–Ω–Ω–æ–≥–æ –±–æ—Ä—â–∞ –®–µ–ª—É—à–∏?\n"
            "‚Äî –ß—Ç–æ —Ç–∞–∫–æ–µ –±—É–∫–µ—Ç –ì–∞—Ä–Ω–∏?\n"
        )

        return system_message
